\section{Trust in P2P networks}
\label{sec:trust_in_p2p}

In comparison with centralized approaches, we know that the reliability of a
centralized identification service depends mostly on the
supporting host: its availability, its security and its capacity  to handle
significant amounts of concurrent requests. 
In practice this entails that a centralized identification service is not
foolproof: it is as trustworthy as the group of servers it runs on.
Well-established centralized identification services have been breached.\\ %%%%%%%%% TODO: ADD REFERENCES


In a decentralized approach, the main issue is that the supporting hosts are
not accountable for their decisions and therefore cannot be trusted.
Since nodes in a P2P network are heterogeneous, some nodes
might be benevolent in providing services. Some might be 
buggy or malicious and cannot provide services with the 
quality that they advertise. Since there is no centralized 
node to serve as an authority to monitor and punish the 
nodes that behave badly, malicious nodes have an incentive 
to provide poor quality services for their benefit because 
they can get away. Some traditional security techniques, 
such as service providers requiring access authorization, or 
consumers requiring server authentication, are used as 
protection from known malicious nodes. However,
% relying in centralized services
%limits the growth of the decentralized network and still
 they cannot prevent from nodes providing variable-quality 
service, or nodes that are unknown.


\subsection{Trust management in P2P networks}
In the absence of trusted nodes in a P2P network, additional mechanisms are
needed to attain a certain level of trust between the participants of a transaction.
 There are reputation systems that grade each node to determine if an
operation. There are also accountability systems that detects malicious
behavior in the nodes in the network. While  detecting malicious behavior with
accountability systems can ward off a variety of attacks, they need to
constantly follow along for each step of the transaction as it happens, with
high message and computational costs.
 In the following section we will 
detail a reputation system and a trust model that will be used in the proposed
solution.


\subsection{Reputation systems}
\label{sec:reputation_systems}
Reputation systems mitigate the problem of malicious nodes in
P2P networks, by building trust among the nodes. The key
idea of a reputation system is to predict the future behaviour
of nodes based on feedback about their past
transactions~\cite{Resnick:2000:RS:355112.355122}. A
transaction is application dependent, for example, forwarding a
message in the network, buying an item in e-commerce services,
sharing or storing files, etc. After a transaction, the client node emits
a recommendation that evaluates the behaviour of the other peer.
The aggregation of these recommendations leads to a reputation
value.
A reputation system built on a DHT has the ability
to compute a global reputation value for every node. Indeed,
all the recommendations about a single node can be handled
consistently at a common location: either by a specific node
or by a set of nodes. Among existing reputation systems for
DHTs we can cite: PeerTrust~\cite{xiong2004peertrust}, WTR~\cite{bonnaire2009wtr},
Eigentrust~\cite{kamvar2003eigentrust} and
PowerTrust~\cite{rahbar2007powertrust}.

Reputation systems deal with malicious nodes that:
do not participate, collude with other malicious nodes, and
emit false recommendations. Nevertheless, to our knowledge,
none of the existing solutions to promote trust in P2P can be
$100\%$ effective in detecting and blocking these attacks.

There is a $5\%$ error assumed in the classification of trusted nodes.

\subsubsection{WTR}
\label{sec:wtr}
WTR~\cite{bonnaire2009wtr} is a reputation system built on a DHT like Pastry~\cite{pastry} and
Chord~\cite{chord}. The probability that a node $X$ is honest is determined by
$R(X)$, with $0 \leq R(X) \leq 1$. WTR calculates the node reputation $R(X)$
using the \textit{recommendations} of the node $X$. When a node $Y$ makes a transaction with
node $X$, node $Y$ issues a recommendation about the behavior of node
$X$. The recommendations table~\ref{table:wtr_recomendations} shows how each
transaction is evaluated.
 
  \begin{table}
    \centering
    \footnotesize
    \begin{tabular}{|l|l|l|}
      \hline
      \textbf{Status} & \textbf{Recommendation} & \textbf{Description}\\
      \hline
      Excellent  & $1.00$    & Very good transaction\\
                &           & Service fully completed\\
      Good      & $0.75$    & Good transaction\\
                &           & Some degradation\\
      Neutral   & $0.50$     & Not completed correctly\\
                &           & Could be independent of the node\\
      Bad       & $0.25$    & Transaction not completed.\\
                &           & Maybe a malicious node\\
      Malicious & $0.00$    & Transaction not completed\\
                &           & Fully malicious node\\
      \hline
    \end{tabular}
    \caption{Recommendations in WTR}
    \label{table:wtr_recomendations}
  \end{table}

A set of $M$ nodes manage the reputation and the risk associated with node $X$.
The node ID of the manager $i$ of the node $X$ is
obtained by the function $m_i = SHA^{i}(X)$. Each manager of the node $X$
stores the list of recommendations emitted about $X$. When a manager receives a
new request for node $X$, it computes the new reputation value and the risk
value for $X$.
Each manager $m_i$ administers a number of $L$ replicas of the reputation information. In Pastry,
the replicas are stored in the leafset of each node, while in Chord the $r$
successors of each node are used.
The reputation and the risk associated with node $X$ are probabilities. The
reputation of $X$ is the probability that $X$ can be trusted. The risk of node
$X$ represents the risk of making a transaction with node $X$. A risk of $1$
means that making a transaction with $X$ is really risky, even if $X$ has a very
good reputation.

The reputation of a node in WTR is influenced by the credibility of the nodes
that emitted the recommendations about node $X$. The credibility of a node $K$ at a
moment $t$ is denoted by $C_t (K)$:%~\ref{eq:credibility_of_a_node}.

\begin{equation}
  C_t (K) = \left\{
  \begin{array}{l l}
    1, & if R_t(K) \in [0.75 \cdots 1]\\
    0.75, & if R_t(K) \in [0.5 \cdots 0.75]\\
    0.2,  & if R_t(K) \in [0.25 \cdots 0.5]\\
    0.1, & if R_t(K) \in [0.0 \cdots 0.25]\\
  \end{array}\right.
%\label{eq:credibility_of_a_node}
\end{equation}

To calculate the reputation of a node, the system only takes into consideration
the last $m$ recommendations in its recommendations list. This window size $m$ is
a fixed parameter of the reputation system. If the recommendation  $i$ emitted
by the node $K$ to the node $X$ is represented by $F_i^k (X)$, then the
reputation of the node $X$ at the time $t$ is defined by:

\begin{equation}
  R_t(X) = \frac{\sum_{i=0}^{m-1}  log(m-i+1) \cdot F_i^k(X) \cdot C_t(K)}{
\sum_{i=0}^{m-1} log(m-i+1) \cdot C_t(K)}
\end{equation}\\


The problem with only considering the reputation of a node when making a
transaction is that:
\begin{enumerate}
  \item A node with a number $n$ of good recommendations, with $n \ll m$ (very
few recommendations), can have a very similar reputation to a node with at
least $m$ recommendations. However, an application could really prefer a node
with a larger history as its reputation is computed with more information, and
is therefore more reliable.
  \item A node that makes good and bad transactions in turns can have a very
similar reputation to a node with more regular behavior. It can then be
preferable for an application to choose a node with more regular
behavior than a very irregular one.

\end{enumerate}

It is for these reason that the \textit{risk} of a node is considered. $J_t(X)$ is
the risk to make a transaction with a node $X$ at time $t$. A risk of 0
means that the reputation for node $X$ is fully reliable.

\begin{equation}
  J_t(X) =  T_1 + T_2
\end{equation}

where

\begin{equation}
  \left\{
  \begin{array}{l}
    T_1 = \alpha \cdot (1-\frac{r}{m}\\
    T_2 = 4 * (1- \alpha) \cdot (1-\frac{\sum_{i=0}^{k} (F_i(X)-\bar{F_i(X))^2})}{r}
  \end{array}\right.
\end{equation}\\

Where $r$ is the number of available recommendations in the sliding window of
size $m$ for node $X$. $\alpha$ is a dynamic parameter within the range of
$[0\cdots 1]$ that allows an application to give more weight to $T_1$ versus
$T_2$ for the computation of risk. $T_1$ is used to evaluate the risk
associated with the number of recommendations taken into account when computing the
reputation of the node $X$. $T_2$ corresponds to the variance of the
recommendations emitted by others nodes. The role of the factor $4$ is to
normalize $T_2$ to obtain a value within the range of $[0\cdots 1]$

Experimental results show that WTR works well when malicious nodes make up less
than $30\%$ of the network, but becomes unstable with more malicious nodes.
It is also resistant to DDOS attacks
thanks to the recursive replication used~\cite{recursive_replication}.

% revise this
\subsubsection{CORPS}
\label{sec:corps}
While reputation systems let you to know how much a node can be
``trusted'', they don't provide a way to find nodes to build trusted services
on. CORPS~\cite{rosas2011corps} considers a probabilistic model of trust based on
reputation to build trusted infrastructure inside a DHT. CORPS builds a double
ring inside inside a DHT. Over this trusted ring a routing algorithm is supplied to
find the most trustworthy node closest to a key $k$.
To be part of the trusted ring, a node needs to have a reputation $R(X) \ge
\rho$, where $\rho$ is a parameter of the system. 

Each node maintains a \textit{trustset} composed by it's $D$ closest trusted
nodes, $\frac{D}{2}$ nodes to it's left and $\frac{D}{2}$ nodes to it's right.
The root node of the trustset is called the \textit{manager} of the nodes that are
in its trustset.

When a new node enters the DHT, the node gathers a trustset from the nodes in its own
leafset. If there is not a trustset available, the node stays temporarily
isolated. The node can recover a trustset later through transactions done with
other nodes in the ring.

When the reputation of a node $R(X)$ gets higher than the system parameter
$\rho$, the node $X$ sends a JOIN message to all nodes in its trustset. Each
node of the trustset verifies that the condition $R(X) \ll \rho$ has been met.
If the condition  applies, each node in the trustset adds $X$ to its own trustset
and forwards this information to all the nodes in its leafset, making them the
\textit{followers} of $X$.
If the condition is not met, $X$ does not enter the trustset and continues
being a normal node.

Normal nodes does not register when they enter the trustset. It is the
responsibility of the new trusted node to notify each node in its leafset about
the new trustset.

Each $\delta T$, the manager's nodes, checks its trusted node's reputation. If
the reputation of a trusted node changes to $R(X) \in [\rho -\alpha, 1]$, then
a REMOVE\_MESSAGE is sent to the followers of $X$. $\alpha$ is a parameter
representing the tolerance of the system. It prevents a yo-yo effect, where the nodes keeps entering and
leaving the trusted ring generating high message costs.

As the system needs a minimum history of the behavior of the nodes transactions
done in the network, an initialization period is needed. During this period,
various locally trusted rings can coexist.
The system stabilizes after approximately five transactions per node, at which
point the trusted ring is fully connected.

To route messages in the ring, CORPS developed pseudo-trusted routing. Pseudo-trusted routing returns the closest trusted node of a given key $k$.

As a way of encouraging nodes to participate in the trusted ring, CORPS gives
priority to those nodes using the pseudo-trusted services.
Also, if a new node enters the system with a reputation value of $R(X) =
\beta$, it needs $R(X) > \beta$ to use the trusted ring services. This is
done to ensure that new nodes use the services of the trusted ring only if they
have made good transactions to raise their reputation. $\beta$ is a static parameter of the
system. 


\subsection{Accountability systems}
Accountability systems detect and expose malicious actions.
To do this, they review each transaction done by the nodes in the system. They
put in disposition of honest nodes the information needed to detect and test
malicious behavior of other nodes. PeerReview~\cite{haeberlen2007peerreview} is a system that
use secure records of the messages sent and received by each node to identify
malicious actions and verify the behavior of a node.

\subsubsection{PeerReview}
\label{sec:peerreview}
PeerReview~\cite{haeberlen2007peerreview} uses the records of the transactions
to detect if a node behavior is malicious or not. It stores secure records with
a list of entries of all the Input/Output made by a node. All this is stored in
a log, which is an append-only list that contains all the inputs and outputs of a
particular node's in chronological order. Each log entry $e_k = (s_k, t_k,
c_k)$ has a sequence number $s_k$, a type $t_k$ and a content $c_k$.
Additionally, each record includes a recursively defined hash value $h_k =
H(h_{k-1} || s_k|| t_k || H(c_k))$; $||$ stands for concatenation. The base
hash $h_{-1}$ is a well-known value.


The resultant hash chain, along with a set of authenticators, makes the log tamper-evident. An authenticator
$\alpha^j_k= \sigma_j(s_k , h_k )$ is a signed statement by node $j$ that its log
entry $e_k$ has hash value $h_k$; $\sigma_j (\cdot)$ means that the argument is
signed with $j$'s private key.

By sending $\alpha^j_k$ to node $i$, a node $j$ commits to having
 logged entry $e_k$ and to the contents of its log before $e_k$. If $j$
 subsequently cannot produce a prefix of its log that matches
 the hash value in $\alpha^j_k$, then $i$ has verifiable evidence that $j$
has tampered with its log and is therefore faulty.
Moreover, node $i$ can use $\alpha^j_k$ as verifiable evidence to convince
 other nodes that an entry $e_k$ exists in $j$'s log. Any node
 can also use $\alpha^j_k$ to inspect $e_k$ and the entries preceding it
 in $j$'s log. To inspect $x$ entries, node $i$ challenges node $j$ to return
 $e_{k - (x-1)}, \cdots , e_k$ and $h_{k - x}$. If node $j$ responds, node $i$ calculates the
hash value $h_k$ from the response and compares it with the
 value in the authenticator. If node $j$ has not returned the correct
 log entries in the correct order, the hash values will differ. In
this case, node $i$ has evidence that node $j$ is faulty. 
 

A problem with this type of systems, is that the actions that can be detected
directly depends of the protocol used. If a malicious node executes an action
that is not specified in the protocol, it will pass undetected. Also, it only
detects and label nodes with malicious behaviors; it does not proportionate a
way to find honest nodes in the network.
Given that transactions must be periodically verified, this mechanism can have a high
CPU cost if the protocol is computationally complex.
 

% description of distributed CA with self-certification
%\subsection{Self-Certification}
%A trusted authority issues identity certificates in a
%centralized system. P. Dewan proposed a self-certification
%mechanism [12] that splits the trusted entity among the
%peers and enables them to generate their own identities in a
%decentralized reputation system. Certified Authority (CA)
%is run by each peer so as to issue an identity certificate(s)
%for itself. These self certified certificates are similar to
%SDSI certificates [9]. Each peer has its own reputation and
%the reputations of all peers collectively form the reputation
%of a CA.
%In Self-certification mechanism there is no need for
%centralized trusted entity which issues identities in a
%centralized system. There is no way to map the identity of
%a peer in the system to its real-life identity when they use
%self-certified identities. They remain pseudonymous in the
%system. The idea of making peers anonymous or
%pseudonymous is desirable in P2P networks, but it can also
%backfire sometimes.
%In Self-certification mechanism the anonymity of the peers
%is preserved by grouping of peers. The combination of self
%certification and anonymity limits the possibility of a Sybil
%attack. In contrast to the traditional CA-based approach,
%neither the group authority nor the transacting peers can
%establish the identity of the peer. In addition, certificate
%revocations are not needed in the group-based approach as
%the group authority only vouches for the real-life existence
%of the peer, unlike the traditional certificate-based
%approaches where various certificate attributes are attested
%by the authority and necessitate revocation if any of those
%attributes mutate in time. If a highly reputed identity is
%compromised, its misuse would be self-destructive as its
%reputation will go down if misused.
