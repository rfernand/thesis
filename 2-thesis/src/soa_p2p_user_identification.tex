
%% introduction of identification schemes. Par 2: proposed username/password identification schemes. Part 3: securing the network protocols. Part 4: Pending issues.


\section{Encrypting content}

\section{Keys generation}
\subsection{Randomly derived}
  manual backup of the keys
\subsection{Keys derived from a password}

\section{Resilience against malicious nodes}

 \subsection{Reputation systems}
Reputation systems mitigate the problem of malicious nodes in
P2P networks, trying to build trust among the nodes. The key
idea of a reputation system is to predict the future behaviour
of nodes based on feedback about their past transactions [1]. A
transaction is application dependent, for example forwarding a
message in the network, buying an item in e-commerce services,
share or store files, etc. After a transaction, the client node emits
a recommendation that evaluates the behaviour of the other peer.
The aggregation of these recommendations leads to a reputation
value.
A reputation system built on top of a DHT has the ability
to compute a global reputation value for every node. Indeed
all the recommendations about a single node can be handled
consistently at a common location: either by a specific node
or by a set of nodes. Among existing reputation systems for
DHTs, we can cite: PeerTrust~\cite{peertrust}, WTR~\cite{wtr},
Eigentrust~\cite{eigentrust},
PowerTrust~\cite{powertrust} and CORPS~\cite{corps}

Reputation systems have to deal with malicious nodes that:
do not participate, collude with other malicious nodes and
emit false recommendations. There are techniques to mitigate
the impact of these attacks, such as the ones presented in
TrustGuard~\cite{trustguard} and WTR~\cite{wtr}. Nevertheless, to our knowledge,
none of the existing solutions to promote trust in P2P can be
$100\%$ effective in detecting and blocking these attacks.

It's assumed an $5\%$ error in the clasification of trusted nodes.


\paragraph{CORPS trust model}
They consider a probabilistic model of trust based on reputation.
The reputation value $R(X)$ is the probability that node $X$ will
be honest in the future. This reputation value is computed
according to a list of recommendations emitted by nodes that
have already carried out transactions with $X$.
After each transaction, a node emits a recommendation
about its peer. A node may lie: it may emit negative
recommendations about a peer that behaves correctly, or
positive recommendations about a malicious peer. Several nodes
may collude to increase or decrease the reputation value of
another node. These problems generate a deviation between
the computed reputation of the node and its real behaviour. It is
considered that this deviation depends on the function used to
compute the reputation value and on the percentage of malicious
nodes within the system.

It assumes a reputation system in the overlay structure with the following properties:
\begin{enumerate}
  \item Every node $X$ has an associated reputation value $R(X)$
  which represents the probability that $X$ is an honest node.
  \item $R(X)$ is computed using the recommendations emitted
  by nodes that have completed a transaction with $X$. Bad
  recommendations have a stronger effect on $R(X)$ than
  good ones. It should be more difficult for node to
  increase its reputation value than to decrease it.
  \item For every node $X$, $R(X)$ is highly available in the DHT.
\end{enumerate}

To avoid nodes that lie about a reputation value, it considers a reputation
system that computes the reputation of nodes concurrently on different nodes.
They decide individually if that node is reputable or not, using a voting
scheme in case of disagreement. On the whole, assuming that there is a smaller
percentage of malicious nodes in the network, the result avoids
false statements about reputation.

To maintain the same nomenclature used in CORPS, in all the following sections,
we call a node $n_i \in TS$ a trusted node, even if there still remains a
probability that this node's actual reputation is smaller than the threshold
$\rho$.
 
 
\subsection{Self-Certification}
A trusted authority issues identity certificates in a
centralized system. P. Dewan proposed a self-certification
mechanism [12] that splits the trusted entity among the
peers and enables them to generate their own identities in a
decentralized reputation system. Certified Authority (CA)
is run by each peer so as to issue an identity certificate(s)
for itself. These self certified certificates are similar to
SDSI certificates [9]. Each peer has its own reputation and
the reputations of all peers collectively form the reputation
of a CA.
In Self-certification mechanism there is no need for
centralized trusted entity which issues identities in a
centralized system. There is no way to map the identity of
a peer in the system to its real-life identity when they use
self-certified identities. They remain pseudonymous in the
system. The idea of making peers anonymous or
pseudonymous is desirable in P2P networks, but it can also
backfire sometimes.
In Self-certification mechanism the anonymity of the peers
is preserved by grouping of peers. The combination of self
certification and anonymity limits the possibility of a Sybil
attack. In contrast to the traditional CA-based approach,
neither the group authority nor the transacting peers can
establish the identity of the peer. In addition, certificate
revocations are not needed in the group-based approach as
the group authority only vouches for the real-life existence
of the peer, unlike the traditional certificate-based
approaches where various certificate attributes are attested
by the authority and necessitate revocation if any of those
attributes mutate in time. If a highly reputed identity is
compromised, its misuse would be self-destructive as its
reputation will go down if misused.
