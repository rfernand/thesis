%\input{src/intros/3-soap2p}

\section{P2P definition}
\label{sec:p2p_definition}

A Peer-to-Peer network (onwards called \textit{P2P}) is a distributed system of big scale. His participants
are called \textit{nodes} and they directly share resources and data, acting
like clients and servers. They are called big scale because they are made to
contain millions of nodes through the Internet. Schollmeier at
el~\cite{conf_p2p_Schollmeier01} define P2P networks as:

\numberwithin{mydef}{section}
\begin{mydef}
Una arquitectura de red
distribuida puede ser llamada una red Peer-to-Peer (P-to-P, P2P, ...) si los
participantes comparten parte de sus propios recursos de hardware (poder de
procesamiento, capacidad de almacenamiento, ancho de banda, impresoras, ...).
Estos recursos compartidos son necesarios para proveer el servicio y contenido
ofrecido por la red (por. ej., compartir archivos o compartir espacios de
trabajo para colaboración). Estos son accesibles por otros nodos (o pares) en forma
directa, sin pasar por otras entidades intermediarias. Los participantes de
esta red pasan a ser tanto proveedor del recurso (servicio y contenido) como
demandante del recurso (servicio y contenido).
\end{mydef}

\section{Peer-to-peer system properties}
\label{sec:p2p_characteristics}

The main properties of a P2P system are:
\begin{enumerate}
    \item \textbf{Scalable}: Can grow \textit{ad infinitum} without compromising its performance.
    \item \textbf{Decentralized}: It does not have a central administration.
    \item \textbf{Heteregeneous}: It is conformed by different devices
connected as \textit{nodes}, with different hardware characteristics each one.
    \item \textbf{Robust}: It is resilient to different types of failures; like
falls and lost of nodes, while maintaining a high data and service availability.
    \item \textbf{Self-organized}: The network structure is maintained and organized by the same nodes that form it, without a manual intervetion needed.
\end{enumerate}

\section{Peer-to-peer System structure}
\label{sec:p2p_estructure}

P2P systems are characterized by do not having a central coordination. Each
peer is independient and has a local view of the system. The global behavior
emerges from the local interaction of its members~\cite{Aberer:2001:PIS:503271.503268}.
%El comportamiento global emerge de las interacciones locales
%de sus miembros, y el sistema debe asegurar que toda información y servicio sea
%accesible a pesar de no existir una coordinación central de la
%misma.
Depending in the system topology, they are categorized as structured or
unstructered P2P networks. The structured P2P networks are characterized by a
strong node structure, like a tree or a ring, resulting in additional costs for
each node to maintain the. In unstructured P2P networks the nodes are organized
by simple graphs.

\subsection{Unstructured P2P Networks}
\label{sec:p2p_unstructured}

\begin{figure}
\center
\includegraphics[width=0.5\textwidth]{img/p2p-unstructured}
\caption{Example of unstructured P2P network}
\label{fig:p2p_unstructured}
\end{figure}

%definition
The unstructured systems does not have a strong structure topology. The nodes
establish links in a semi-arbitrary way. Usually, they organize as a hiearchical or a
plain graph.

%advantage
Their main advantage is that they can achieve certain grade of node anonymity.

%disadvantage
The downsize of this type of systems is that searches can end in false
negatives and have a bigger cost in comparizon with structured networks. This
is because the search algorithms do not pass through every nodo inside the
graph.

They are commonly used to share files, using the anonymity of the user to
distribute any tipe of file through the net.

% examples
There are many implementations of unstructured networks:
Gnutella~\cite{oai:CiteSeerXPSU:10.1.1.61.7302}, 
BitTorrent~\cite{bittorrent}, 
%Freenet~\cite{freenet}, 
%Overnet~\cite{overnet}, 
among others. Each one use differents ways to organize the nodes, some using a
node hierarchy to facility the searches in the the network, and others simply
does not count with a search method (BitTorrent).

\paragraph{P2P Unstructured Data Storage}
\label{sec:p2p_unstructured_storage}

This systems storage system does not relate with their network topology, and
does not have a fixed procedure for it. In general, when a node asks for data
and copy it, the network use his copy to distribute it with more nodes in the
system. Each user can decide if he want to share a data or not with the system.
That can make some data of the system go unavailable, and the network by its
own does not have a way to control this.


\paragraph{P2P Unstructured Data Search}
\label{sec:p2p_unstructured_search}

There are many ways to make search inside this type of networks. Some of them
are:

\begin{itemize}
    \item \textbf{Flooding}: 
Each node tries to forward every message to every one of its neighbours except
the source node. To avoid som wasted duplicate deliveries and infinite loops,
each message has a \textit{Time To Live} (TTL) that limits the amount of times
that it can be forwarded. This type of searches has a cuadratic cost of network
messages, of order O($N^2$), were N is the number of nodes that can route data
inside the network.
    \item \textbf{K-Random Walk}: Similar to the \textit{flooding} method, with
the difference that each node chooses a number K or a fixed percentage of his
neighbours and forwards the message to the ones that were chosen. When a node
with the required information is found, it responds with a QUERY\_RESPONSE
message following the same way that the Random Walk used to reach him, back to
the original node that started the search. The message cost of this searches
depends of the value of K, approaching:

\numberwithin{equation}{subsection}
\begin{equation}
\label{eq:krandomwalk}
 C = 2 + TTL +
\sum_{i=0}^{TTL} K^i
\end{equation}

\end{itemize}

A way to reduce the message cost in this type o searches, \textit{caching}
techniques can be used, saving the location of the files shared in the network
as they are distributed in it.

\subsection{P2P Structured Networks}
\label{sec:p2p_estructured}

\begin{figure}
\center
\includegraphics[width=0.5\textwidth]{img/p2p-structured}
\caption{Example of structured network}
\label{fig:p2p_estructured}
\end{figure}


%definition
The structured networks have a strong topology structure. While most of them
use ring structures, other base his topology in binary trees or other
structures to organize.

%advantages
The main advantage of this systems are they searches. The data search does not
suffer of false negatives, and is more efficient in message cost and response
time than the unstrucutred ones. 

%disadvantages
As the nodes can be found and traced easily with a query, user anonymity is harder to
achieve in this type of systems.  The applications that want to maintain user
anonymity from his users, this is one of the main reasons to not use this type
of systems.

% examples
The vast mayority of structured P2P systems use distributed hash tables
(DHT)~\footnote{http://en.wikipedia.org/wiki/Distributed\_hash\_table} as
structural base for the search and storage system~\cite{BalakrishnanEtAl03}.
CHORD~\cite{conf:hotos:DabekBKKMSB01},
PASTRY~\cite{oai:CiteSeerPSU:441779} and 
OpenDHT~\cite{Rhea:2005:OPD:1080091.1080102}
are some of the systems that are DHT based.

\subsubsection{Chord}
\label{sec:chord}

Is a protocol for the implementation of DHT networks that relates a key with
each node. It is design to work in decentralized networks without the use of
special nodes (like super-nodes to help the realization of queries). His
architecture is made to have a basic operational cost proportional to $log(N)$,
with $N$ being the number of nodes in the network.

The protocol takes in consideration that nodes can be inactive or away one
moment or another. Uses the SHA-1 function to asign identifiers to each node
and keys in the system. The Chord protocol dynamically allocates identifiers to nodes that
join or leaves the network. The rest of tasks and services that the system
would need - like user identification, storage, interfaces, etc.- are
responsability of other implementations of a higher abstraction level than the
Chord protocol itself.

\label{sec:chord}
\begin{figure}
\center
\includegraphics[width=0.5\textwidth]{img/chord-search}
\caption{Chord Routing}
\label{fig:p2p_estructured_chord_search}
\end{figure}

\paragraph{Chord Data Storage}
The network data is stored in the closest node succesor of the hash key of the
data. The network searchs are made querying the closest succesor
nodes that the node has in his \textit{finger table}.
The id key $k$ is assigned to the node id $k$ if this one is active in the
network. If $k$ is not available at that moment, it search for the first node
successor to $k$ that is active and the key id is assigned to him. This
sustitute node is denominated \textit{succesor of $k$}.
The more close a node id is from other, the more far away phisically are the
nodes distributed, giving the network a better resilience to errors.

\paragraph{Chord Information Search}
Key-value pairs are generated using a hash function over each participant of
the network.  Each node register a routing table called \textit{Finger Table}.
In the \textit{Finger Table}, each node register his $i$ successor nodes, each
one being the closest successor to $\theta + 2^{i-1)}$. 
In a search, the number of nodes that a query needs to reach is of order
$O(log(N))$.
The problem that Chord has is that it does not maintain a correlation
between the logical locality of the nodes and their geographical positioning. This
produces that the network communication ocurrs between nodes very close
logically, but very far phisically in the network. While some
optimizations using different heuristics exists to make it better, the locality
problem is not completelly fixed.

\subsubsection{Pastry}
\label{sec:pastry}
Pastry es un protocolo para la implementación de una DHT. Éste solo define como
distribuir las claves entre los nodos y como encontrar el nodo responsable de almacenar una clave.

\paragraph{Almacenamiento de datos}

El almacenamiento de datos en los sistemas estructurados tipo DHT, se realiza
aplicando la misma función de hash utilizada para asignar identificadores de
cada nodo (\textit{nodeID}), al nombre del archivo a ser almacenado. Esto resulta en una
clave, la cual es mapeada por el sistema a un único nodo. En el caso de Pastry,
este nodo posee el nodeID numéricamente más cercano a la clave del archivo.
Dicho nodo, será el encargado de almacenar el archivo.

Existen técnicas de replicación que permiten distribuir copias del archivo en
la red, haciendo que los vecinos más cercanos al nodo también guarden el
archivo. De esta forma, si el nodo que debe almacenar la información pierde
conexión, el archivo aún puede rescatarse. Gracias al algoritmo de
asignación de los nodeID, las réplicas cercanas en la red realmente se
encuentran distribuidas lejos geográficamente, aumentando la disponibilidad de
las mismas en el sistema.

%\begin{figure}
%\center
%\includegraphics[width=0.65\textwidth]{img/kademlia_dht_example}
%\caption{Ejemplo de distribución del DHT en Kademlia}
%\label{fig:p2p_estructured_kademlia}
%\end{figure}

\paragraph{Búsqueda de la información}

%La búsqueda de información depende del protocolo de red utilizado.
 En el caso de Pastry, para poder buscar archivos de forma eficiente, cada nodo mantiene un
estado con tres conjuntos de información: una tabla de rutas, un conjunto
leafset y un conjunto de vecinos.
El leafset es el conjunto de nodos más próximos al nodo local en las dos
direcciones del círculo, y sirven para mantener la coherencia de la red y acortar las
búsquedas. Los nodos vecinos son los $m$ nodos más próximos según las
métricas usadas en la red, que aunque no son usados por el algoritmo de
enrutamiento, sirven para mantener la tabla de rutas.

La tabla de enrutamiento contiene una fila por cada bloque de direcciones
asignado. Los bloques se forman dividiendo el nodeID del nodo local en dígitos
de $b$ bits cada uno, generando un sistema numérico en base $2^b$. De esta forma, respecto al cliente,
agrupamos los nodos según el número de dígitos comunes en un prefijo del nodeID
del nodo local y otro nodo. La tabla almacena en cada fila la dirección del nodo conocido más
cercano y que cumple la condición del prefijo correspondiente a esa fila.

Los mensajes pueden ser dirigidos a cualquier dirección en el espacio de
claves, exista el nodo con esa ID o no. El contenido se va trasladando por al red hasta llegar al
nodo con la ID dada o, si no existe, al más próximo. Cuando un nodo recibe un mensaje para
reenviar o envía uno, consulta la tabla del leafset buscando una ruta directa. Si no la
tiene, consulta la tabla de enrutamiento buscando el nodo conocido con el mayor prefijo en
común con el objetivo, y le envía el mensaje. Esto asegura que el mensaje en cada paso vaya
a un nodo con un nodeID más cercano al destino (o en el peor de los casos, igual de cercano)
con lo que la operación converge.

% En \ref{fig:p2p_estructured_routing_table} se puede
%observar un ejemplo de la información que posee un nodo en Pastry.

%\begin{figure}
%\center
%\includegraphics[width=0.4\textwidth]{img/pastry-routing-table}
%\caption{Tabla de información de ruteo de un nodo en Pastry}
%\label{fig:p2p_estructured_routing_table}
%\end{figure}

 En \ref{fig:p2p_pastry_routing} se puede
observar un ejemplo del algoritmo de enrutación de un mensaje en Pastry.

\begin{figure}
\center
\includegraphics[width=0.6\textwidth]{img/pastryrouting}
\caption{Ruteo de un mensaje en Pastry}
\label{fig:p2p_pastry_routing}
\end{figure}


La búsqueda se basa en la función
\textit{lookup(key)}~\cite{BalakrishnanEtAl03}. Esta función, toma una
clave (\textit{key}) y busca dentro de la red un nodo cuyo nodeID es
numéricamente más cercano a la clave.  Si el dato existe en la red, la función
lookup retorna el nodo que lo posee.  Una característica importante de las
búsquedas en los DHT como Pastry, es que el costo en número de mensajes, se
mantiene en el orden $O(log(N ))$, donde $N$ es el número total de nodos en el
sistema. Además poseen la propiedad de \textit{decidibilidad}, lo cual significa
que se puede asegurar que, si un dato existe en la red, éste puede ser
encontrado en caso de ser solicitado por algún nodo. Otra característica es el
balanceo de carga que posee el sistema. El ruteo asegura
que la consulta pasará por un nodo del leafset antes de llegar al nodo raíz,
permitiendo que el nodo del leafset pueda responder a la consulta, balanceando naturalmente las
consultas entre los nodos pertenecientes al leafset del nodo raíz.



%
%
%\subsubsection{CAN}
%\label{sec:can}
%%The “Content Addressable Networks” (CAN) [22] work is being done at AT&T Center for Internet Research
%%at ICSI (ACIRI). In the CAN model, nodes are mapped onto a Æ -dimensional coordinate space on top of
%%TCP/IP in a way analogous to the assignment of IDs in Tapestry. The space is divided up into Æ dimensional
%%blocks based on servers density and load information, where each block keeps information on its immediate
%%neighbors. Because addresses are points inside the coordinate space, each node simply routes to the neighbor
%%which makes the most progress towards the destination coordinate. Object location works by the object
%%server pushing copies of location information back in the direction of the most incoming queries.
%%There are several key differences between CAN and Tapestry. In comparison, Tapestry’s hierarchical overlay
%%structure and high fanout at each node results in paths from different sources to a single destination con-
%%verging quickly. Consequently, compared to CAN, queries for local objects converge much faster to cached
%%location information. Furthermore, Tapestry’s use of inherent locality paired with introspective mechanisms
%%means it allows queries to immediately benefit from query locality, while being adaptive to query patterns
%%and allowing consistency issues to be handled at the application layer. CAN assumes objects are immutable,
%%and must be reinserted once they change their values. Finally, Tapestry node organization uses local net-
%%work latency as a distance metric, and has been shown to be a reasonable approximation of the underlying
%%network. CAN, however, like Chord, does not attempt to approximate real network distances in their topol-
%%ogy construction. As a result, logical distances in CAN routing can be arbitrarily expensive, and a hop
%%between neighbors can involve long trips in the underlying IP network. The main advantage a CAN has is
%%that because of the simplicity of the node addition algorithm, it can better adapt to dynamically changing
%%environments such as sensor networks.
%%In summary, Pastry, Chord and CAN are very similar to Tapestry in their functionality and run-time proper-
%%ties. In particular, Pastry is the closest analogy offering “locating and routing” to an object, where Chord and
%%CAN both focus on providing distributed hashtable functionality. Because Pastry controls replica placement,
%%and Chord and CAN are not optimized for large objects, Tapestry is the only system which allows the user
%%to control the location and consistency of the original data, allowing the system to manipulate and control
%%only references to the object for performance. It is also noteworthy that Tapestry and Pastry have natural
%%correlation between the overlay topology and the underlying network distance, while CAN and Chord may
%%incur high physical hop counts for every logical hop.
%
%\begin{figure}
%\center
%\includegraphics[width=0.6\textwidth]{img/can_structure}
%\caption{Ejemplo de la estructura de CAN}
%\label{fig:can_structure}
%\end{figure}
%
%\paragraph{Almacenamiento de datos}
%En CAN (\textit{Content Addressable Networks}) el espacio de nombres es
%dividido en bloques dimensionales~\ref{fig:can_structure} basándose en la densidad y carga de
%información de los nodos. CAN asume que los objetos son inmutables, y deben ser reinsertados
%una vez que han cambiado sus valores.
%
%\paragraph{Búsqueda de la información}
%Cada bloque mantiene información sobre sus vecinos
%inmediatos. Debido a que las direcciones son puntos dentro del espacio de
%coordenadas, cada nodo simplemente enruta hacia el vecino que realiza el mayor
%progreso frente al del vecino. Una vez que el objeto es encontrado, la
%información es enviada de forma inversa hacia la dirección de más consultas
%realizadas. Al igual que Chord, no mantiene
%correlación entre el espacio de nombres utilizado y la distancia física de los
%nodos, pero su simplicidad le permite adaptarse mejor a ambientes de alto
%dinamismo.
%
%%\subsubsection{Skip Graphs}
%%\paragraph{Almacenamiento de datos}
%%\paragraph{Búsqueda de la información}
%
%\subsubsection{Tapestry}
%\label{sec:tapestry}
%
%%PAST [11] is a recent project begun at Microsoft Research focusing on peer-to-peer anonymous storage.
%%The PAST routing and location layer, called Pastry [10], is a location protocol sharing many similarities
%%with Tapestry. Key similarities include the use of prefix/suffix address routing, and similar insertion/deletion
%%algorithms, and similar storage overhead costs.
%%There are several key differences that distinquish Pastry from Tapestry. First, objects in PAST are replicated
%%without control by the owner. Upon “publication” of the object, it is replicated and replicas are placed on
%%several nodes whose nodeIDs are closest in the namespace to that of the object’s objectID. Second, where
%
%%Tapestry places references to the object location on hops between the server and the root, Pastry assumes
%%that clients use the objectID to attempt to route directly to the vicinity where replicas of the object are kept.
%%While placing actual replicas at different nodes in the network may reduce location latency, it comes at the
%%price of storage overhead at multiple servers, and brings with it a set of questions on security, confiden-
%%tiality, and consistency. Finally, Pastry routing’s analogy of Tapestry’s “surrogate routing” algorithm (see
%%Section 3.3) provides weaker analytic bounds on the number of logical hops taken. In Tapestry, we have
%%analytically proven, well-defined, probabilistic bounds in routing distances, and are guaranteed to find an
%%existing reachable object (see Section 3).
%
%\paragraph{Almacenamiento de datos}
%Comparte muchas similitudes con Pastry, como su ruteo basado en el
%prefijo de las direcciones, algoritmos similares para la  inserción y borrado y
%costos de almacenamiento de la información.
%Tapestry usa claves numéricas de 160 bits, generadas mediante SHA-1, como
%identificadores tanto de nodos como de objetos dentro de la red. Éstos
%identificadores suelen representarse en formato hexadecimal, siendo más
%próximos dentro del espacio de identificadores contra más dígitos tengan en
%común. 
%Los objetos son publicados enviando un mensaje de publicación hacia el nodo
%raíz correspondiente al hash del objeto. Cada nodo del camino guarda un puntero
%hacia el objeto. Los links redundantes son priorizados por latencia y/o
%localidad.
%
%%De ésta forma, objetos son encontrados realizando consultas hacia la
%%raíz del objeto, en donde cada nodo por el camino revisa sus punteros y
%%redirige la petitición apropiadamente.
%
%%Participants in the network can publish objects by periodically routing a
%%publish message toward the root node. Each node along the path stores a pointer
%%mapping the object. Multiple servers can publish pointers to the same object.
%%The redundant links are prioritized by latency and/or locality. Objects are
%%located by routing a message towards the root of the object. Each node along
%%the path checks the mapping and redirects the request appropriately. The effect
%%of routing is convergence of nearby paths heading to the same destination.
%
%
%\paragraph{Búsqueda de la información}
%
%Un mensaje busca primero un nodo cercano que tenga en común con la clave
%del destino el mismo número en el dígito de menos peso, incrementando
%paulatinamente la parte común hasta encontrar el nodo existente más cercano,
%como se puede visualizar en la figura~\ref{fig:bayeux_routing}.
%
%Para dirigir los mensajes a su destino, Tapestry usa tablas de enrutamiento
%locales en cada nodo, conocidas como \textit{neighbor maps}. La tabla contiene
%múltiples niveles, en donde cada nivel representa un sufijo coincidente hasta
%un dígito. Un nivel dado contiene varias claves, las cuales varían solo en el
%número en la posición indicada por la tabla, similar a la tabla de ruteo
%utilizada por Pastry.
%
%Una búsqueda realiza $O(log_B N)$ saltos en una red de tamaño $N$ y espacio de
%nombres de base $B$. Para un mejor nivel de tolerancia de fallos, se mantiene
%una lista de $c$ links secundarios, de tal forma que la tabla de ruteo posee
%un tamaño de $c B log_B( N)$.

%\red{OTRAS REDES}


\subsection{P2P Networks and User Identification systems}

Debido a las propiedades poco favorables de las redes P2P no-estructurados para la implementación de una red
social, una red P2P estructurada como base sería lo más adecuado para su
implementación, por lo que el estudio a continuación se centrará  principalmente
en los sistemas Peer-to-Peer estructurados.

\subsubsection{Implementaciones de redes sociales P2P}
No existen actualmente implementaciones de redes sociales P2P,
existiendo sólo propuestas teóricas para una implementación futura. Un proyecto
que actualmente se encuentra trabajando para lograr esto es
PeerSoN~\footnote{http://www.peerson.net}~\cite{buchegger:peerson, buchegger:2009:pps:1578002.1578010}.

\paragraph{PeerSoN}
PeerSoN presenta un prototipo de estructura para redes sociales
Peer-to-Peer, buscando asegurar la privacidad de los usuarios y potenciar la
comunicación directa entre cada usuario.
Su arquitectura está basada en 2 capas, una para la búsqueda y otra para el almacenamiento de la
información. Considera la asignación de identificadores únicos para cada
usuario, procedimientos de entrada, envío y obtención de archivos y el manejo
de mensajes asincrónicos.

La capa de almacenamiento consiste en los peers en sí, los cuales, una vez
encontrados por la capa de búsqueda, pasan a juntar y enviar la información
directamente entre sí, distribuyendo réplicas de éstos
en la red para aumentar la disponibilidad de los datos. 
PeerSoN~\cite{buchegger:peerson} basa su seguridad en cifrado simétrico y asimétrico. La
información primero es cifrada usando una llave simétrica, y luego esta
llave es cifrada con las llaves públicas de los recipientes. Los
identificadores de los usuarios son cifrados junto con las llaves
simétricas, siendo todo enviado con la información cifrada.

 Por último, para la capa de búsqueda usan las capacidades de almacenamiento,
ruteo y recuperación de información de
OpenDHT~\cite{Rhea:2005:OPD:1080091.1080102}.

%#peerson
Dentro de sus debilidades se encuentran un sistema de seguridad y de búsquedas
muy básicos, sin abordar las problemáticas que pueda tener en una
implementación real del sistema. Su sistema de cifrado requiriere para poder compartir
un dato a grupo una cantidad de procesamiento proporcional a la
cantidad de usuarios. Además, en caso de modificación del grupo, se requeriría
el recifrado de todos los archivos compartidos entre sí, con todos los
costos que esto significaría. En el caso del sistema de búsqueda, al basarse en
un DHT sin mecanismos adicionales, no permitiría búsquedas complejas o de más
de una dimensión, dificultando el proceso de establecimiento conexiones entre
la red.

\paragraph{Safebook}
Safebook~\cite{conf_wowmom_CutilloMO11} es un propuesta la formación para una
red social que utiliza dos principios de diseño: descentralización y uso de la
confianza con los usuarios de la vida real. Su arquitectura la organiza
separándola  en 3 niveles: red social (\textit{social network}, SN), aplicación
(\textit{application services}, AS) y transporte y comunicación
(\textit{communication and transport}, CT). El primer nivel es provisto por la
representación digital de los miembros y sus relaciones, la capa da aplicación
corresponde a la infraestructura de la red social y por último la capa de
comunicación que corresponde al Internet. 
De ésta forma, un usuario es representado como un \textit{host node} en la
Internet, un \textit{peer node} en la capa P2P, y un miembro o usuario en la
red social.
Uno de sus objetivos es ser resistente a gran parte de los posibles ataques a
redes sociales~\ref{sec:ataques}.

 Confía en matryoshkas que proveen el almacenamiento de la información, perfiles de la obtención de la información
y ofuscación de la comunicación. Una matryoshka consiste en un set de nodos
agrupados en varios anillos concéntricos, los cuales se organizan acorde al nivel de confianza que el
nodo, asociado con la matryoshka, tiene hacia los nodos de cada anillo. El
anillo más interior es el más confiado y estaría formado por los ``amigos'' del
nodo. Este sería el responsable  de almacenar la información replicada del nodo
asociado con la matryoshka. Las capas más cercanas almacenan la información
publicada de forma encriptada y no-encriptada, pero la información privada es
almacenada por el mismo dueño y no es replicada hacia los anillos. 

Para la autenticación del usuario, utiliza un servicio de identificación
confiable para proveer a cada nodo identificadores únicos: un \textit{identificador del
nodo} y un \textit{seudónimo}.
Acuerdo a~\cite{springerlink_10.1007_978-3-642-14282-6_7}, un esquema simple de encriptación grupal es usada para la
encriptación, y usuarios obtienen llaves oportunamente para desencriptar la
información publicada. El dueño debe explícitamente autorizar y volver a publicar al
anillo interior cada mensaje escrito por otros usuarios.
La anonimidad en Safebook es obtenida usando un ruteo de múltiples saltos. Una
tabla de hash distribuida mantiene punteros a los nodos pertenecientes al
anillo más lejano de la matryoshka. Las peticiones entrantes son ruteadas desde
el anillo más lejano hacia el centro de la matryoshka. Los mensajes son
encriptados en cada salto usando criptografía asimétrica.

La búsqueda de información se realiza a través de consultas recursivas en el
sistema P2P a través del DHT. Para ello, el nodo realiza una consulta
por la información del usuario objetivo, requiriendo el conocimiento previo de
los identificadores de éste. De ésta forma, consulta al nodo responsable del
identificador de ese usuario por la lista de nodos de entrada de la
capa más lejana de la matryoshka del usuario objetivo. Por último, la consulta
prosigue reenviandose a través de los nodos de cada capa de la matryoshka, hasta
llegar al centro de la misma. La información requerida llega de vuelta a través
invirtiendo el camino de llegada de la consulta.
No contempla un sistema de búsqueda adicional al provisto por el DHT.

Los problemas principales detectados de este sistema se encuentran en la
disponibilidad de la información, problemas de escalabilidad al depender de
sistemas externos para el ingreso de nuevos usuarios a la red y carencias en
los métodos de búsqueda y recuperación de la información.
Esto es debido a que el almacenar réplicas de los datos personales entre los amigos más cercanos
puede generar problemas de disponibilidad de los datos en los momentos en que
éstos no se encuentren en linea. Para la identificación requiere de un servicio
externo centralizado  que pueda ser confiado para la asignación de ID únicas.
Por último, la transmisión de mensajes a través de la red se realiza
anonimizando al que envía y recibe el mensaje, aumentando el costo y tiempo de
envío de cada dato por cada salto que de entre nodo y nodo.
