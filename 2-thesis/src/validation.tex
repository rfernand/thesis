In this chapter we will discuss some security issues and the portability of our
solution.% and our decision about not having a password recovery mechanism

\section{System validation}

In comparison with centralized approaches, we know that the reliability of a
centralized identification service depends mostly on the
supporting host: its availability, its security and its capacity  to handle
significant amounts of concurrent requests. 
In practice this entails that a centralized identification service is not
foolproof: it is as trustworthy as the group of servers it runs on.
Well-established centralized identification services have been breached. %%%%%%%%% TODO: ADD REFERENCES
In a decentralized approach, the main issue is that the supporting hosts are
not accountable for their decisions and therefore cannot be trusted.\\

Because of this, as other P2P systems, a 100\% secure implementation cannot be achieved
(section~\ref{sec:pseudo-secure}).
%  Is the system valid?  fue demostrada la hipotesis?
Taking that into account, the goal of our approach was to drastically reduce the probability of a
malicious node impersonating another user who is using the identification
system. This was done using of a reputation system and trusted node management
to mitigate the effectivity of malicious nodes on identity usurpation attacks.

%% Then... the hypotesis was valid or not?
% cual es la peor probabilidad del sistema?
Our analysis shows that the highest probability of a protocol failing without using trusted nodes is 
with $L=8$, $p = 0.188$, and that the highest probability of a protocol failing
using trusted nodes is with $L=8$, $p = 6.64 \times 10^{-5}$. With $L=32$ the probabilities seems better, but without the use
of a trustset the probabilities remain at $p = 0.016$, too high to be usable in
reality. That is because in a system with $1000000$ users, with the probability
of a protocol failure being $p = 0.016$,  $16000$ of the users will have problems with
the protocols of the system. Then, a solution without the use of trusted nodes
is not acceptable with values of $L <= 32$. In comparison, using
only trusted nodes with $L=32$ we obtain a probability of $p =8.24 \times
10^{-14}$, meaning that in a system with $1000000$ users,
only $0.00000000824$ would fail.
% es aceptable o no? 
Then, using the trusted nodes for the system protocols we can be sure that failures in the
system's identification protocols are very unlikely to happen, being a valid
solution for solving the problem at hand.
% las probabilidades y evaluaciones ahcen el sistema valido o no?...IS VALID OR NOT??
With that, we can conclude that present solution is acceptable to be
used in real life applications.


% Al aumentar L, aumenta la cantidad de mensajes (n^2), es aceptable?
%% La cantidad de mensajes enviados durante los protocolos de registro de
%% usuario y de inicio de sesiÃ³n depende en gran parte del valor de $L$.
%%  ....
If we analyze the message overhead of our protocols, we find that many depends
greatly in the value of $L$. $L$ is a fixed in the network and is
independent of the network size, so we can conclude that the messages costs
remains scalable with an message overhead of the order of $O(log(N))$.
 Still, high values of $L$ can generate a very high constant message cost. For that reason, the leafset size
$L$ should not be higher than $48$.  The implementation of a real life system
should take into consideration the amount of messages sent over the network
during an user session, as the high cost of our protocols, like the user
registration protocol and sign in protocol, can be justified when they represent just a
small fraction of all the system messages.

% cual seria el valor de L?
So, with our solution, with a leafset size of $L = 32$ nodes and a
$5\%$ proportion of malicious nodes distributed uniformly in the P2P system, the resulting probability of a false
positive, where a decision made by malicious nodes is adopted, is of the order
of $10^{-14}$. 
This value is far lower than any we have encountered in the
literature about decentralized identification systems. Also the system
maintains a cost scalable when the size of nodes $N$ of the DHT
increases. In terms of communications overhead, there is a trade off between its
cost and the security provided by the protocols. Using values of
$L = 16$ reduces the message overhead in the system and still provides a decent
probability of error of order $10^{-8}$, which in systems with a low presence of
malicious node can be acceptable. Then, if priority of the system is its
security and the message cost of the protocols is not
an issue for the system that will use it, a value of $L = 32$ is highly recommended. 


\section{Portability}

%% DHT -> Chord u otro?
Although our implementation relies on Pastry as its DHT, our approach can rely
on other building block implementations. For example, it is very simple to
switch the underlying DHT from Pastry to Chord, as Chord maintains structures
that are equivalent to the leafset and the routing table in Pastry, like using
the $r$ successors of Chord instead of the leafset of Pastry.

%% WTR -> Por otro? que valores necesita?
Our implementation can be built over any trust management service that
provides a way to find trusted nodes based on a reputation system
that provides a value $R(X)$, the value that represents the probability of a node
$X$ being honest, and maintains the structural properties of the underling
DHT. For example, WTR~\cite{bonnaire2009wtr} and
PeerTrust~\cite{xiong2004peertrust} are reputation systems that fulfill our
system requirements.

%system with the same properties explained
%in Section~\ref{sec:building_blocks}  is also required, but can be easily
%supported by any type of DHT, as long as it provides some type of key lookup
%service and.
%% CORPS -> explicar por que se usa la probabilidad 0,3 y 0,... en la parte de evaluacion


\section{Security issues}
%Regarding P2P networks and identification systems, there are many issues that
%hand that need to be accounted for when building the system. 

\subsection{Sybil attacks}
\label{sec:sybil_attacks}

The Sybil attack~\cite{the_sybil_attack} is a major concern if obtaining
network identities is cheap (NodeIDs). Such an attack allows to tampers with the ratio of
malicious nodes in the system as a single node acts under multiple identities:
the ratio can rise above the $30\%$ upper threshold. \\

A common feature of the Sybil attack is a single malicious user creating 
several identities with numerically close nodeIDs to be able to control a
substantial part of the DHT. We assume that the generation of the nodeIDs can
be verified by any node in the network, for instance the bootstrapping node or
the node with the closest nodeID. Our proposition assumes that nodeIDs are
computed from a public key and generated with a well-known public/private key
algorithm. We also suppose that the number of bits of these keys is sufficient
to avoid a brute force attack to reverse the SHA-1 function. During the
procedure to join the DHT, a node $A$ must provide several pieces of information to
the verifier node $X$: its public key $key_{pub}$, and the associated nodeID
are computed using
$nodeID = SHA1(key_{pub})$. $X$ can easily verify the correctness of the nodeID
with the public key. If they do not match, then $X$ prevents $A$ from joining
the ring.\\

Due to the mathematical properties of the SHA-1 hash function, it is
improbable that a node will find two keys $key_{pub1}$ and
$key_{pub2}$ such that $SHA1(key_{pub1}) = SHA1(key_{pub2})$. Therefore, it is
also doubtful that a node will find several public keys that give
numerically close nodeIDs in the ring. Thus, it is nearly impossible for a
node to control a substantial sector of the ring.\\

However, a single node can still generate several nodeIDs distributed over the
ring. This kind of Sybil attack does not allow a node to control a sector of the ring,
but it will increase failures in the DHT (routing failures, etc.).  Reputation
systems can efficiently mitigate Sybil attacks, but will collapse if the
ratio of malicious nodes is too high ($> 30\%$). To our knowledge, no existing reputation system can control a large
number of Sybil attacks in a DHT. For this reason, this is still a much
discussed topic within
the P2P research community.\\

As seen before in section~\ref{sec:challenges_puzzles}, other solutions to
mitigate the presence of Sybils is to make obtaining nodeIDs
exponentially difficult when a node needs a new
one. Existing solutions usually
rely on a set of servers to provide the nodeIDs, and are therefore not very
scalable.\\

The impact of a Sybil attack over the trusted ring of the reputation system
would mainly be at the level of its \textit{trustsets}. The main idea is to be
able to control various nodeIDs in a \textit{trustset}. This is not an easy
task: as shown above it is unfeasible to obtain numerically close
nodeIDs that belong to the same \textit{trustset}. Moreover, controlling a very
large number of trustsets is practically impossible, as it will require 
control of a very large number of nodeIDs. This is unfeasible due to the number of
transactions required from an attacker in order to establish a good reputation
for all of its nodeIDs, and then insert them into the Trusted
Ring~\cite{rosas2011corps}.\\

Analog to the problem of Sybil attacks to control multiple nodeIDs, an attacker
would likely try to control a large amount of user accounts in the
identification system. Having registered a large amount of usernames, a new
user could have difficulties in finding an unused username when registering in the
network. Also, having a large amount of identities through the services in the
network can be used to exploit some of their functionalities.
In our solution, we use computational challenges to mitigate the problem of a
node registering multiple users with the system. Another solution to
this problem is the use of CAPTCHA~\cite{von2003captcha} as an additional
challenge for user registration. While this type of challenge is not
without faults, it adds an additional layer of security against Sybil attacks.


\subsection{Collusion of nodes}

A collusion of nodes consists of an agreement made between various nodes to make
a combined attack, like the nodes of the reputation system colluding to artificially increase or decrease the reputation of a
set of nodes. Some existing
reputation systems, like WTR, can efficiently mitigate the effect of this kind
of attack. However, colluding nodes are able to sustain the attack
permanently in order to maintain a good reputation and remain in the trusted ring.
This will have a significant cost for the attackers, and for them to being able to manage a
great number of nodes in the trusted ring is very unlikely. 

\subsection{Offline attacks}
As our solution relies on a public/private key scheme to identify the user,
using derived keys from a public salt stored in the network a malicious node
could try to guess the password of a user. If the function used to derive the
private key has a low computation cost, a brute force attack that tries
different passwords and tests the derived private key with the user public key
can succeed if the attacker has the computational resources to do so. We
assumed that the key derivation function (e.g., bcrypt) is slow enough to make
it unfeasible for a computer to guess a user password in a reasonable 
amount of time. (more than 20 years)

\subsection{Passwords}
The traditional view is that passwords should be replaced by some better
mechanism, but despite significant research efforts into dislodging passwords,
they are still by far the most common authentication mechanism today.
Reasons for their prevalence include simplicity, price, and very strong user
familiarity.
It is known that the average user tends to use the
same password in more than one service, presenting serious security problems
for user identity integrity. A definitive solution to this problem has not
been found yet, but a strict password creation policy can mitigate this
problem. At the same time, this can also increase cases of forgotten passwords.

%\section{Password recovery}
%While the system does not allow password recovery operations...



%there can a time
%when the group nodes cannot decide over something in common, with the
%When commiting a transaction that needs a group of $n$ nodes
%
%
%This causes that after a transaction that
%involves a group of nodes, there is some amount of time where the nodes are not
%syncronized.

\subsection{Node data inconsistency}
Synchronization between nodes does not happen instantly. Also, thanks to
the different routes used in the network, simultaneous messages can reach a
destination at different times. So, as opposed to centralized systems, P2P
networks have certain times when the data in some nodes are inconsistent.
This is relevant for functions that need a
group of nodes to be synchronized at a certain time, because this ``temporal
inconsistency''  can cause fake positives or fake negatives in the protocols of
the network. For example, after a successful password change request, there can
be a time when some nodes will have the old user information. If another
request involving the user credentials is issued before these nodes have
synchronized their data, depending on what nodes are used in the request, the
validation that needs $\frac{L+1}{2}$ identical answers can fail or succeed.
While the timeframe when the nodes have not undergone synchronization is small,
it can still lead to problems if it is not considered in a protocol's
construction. In consequence, an added time span is considered for the
successful completion of the user registration and password change operations.

%To mitigate this problem, we propose the use of an active user information store
%maitenance.  and add consider a timespan of 1 to 2 seconds  


